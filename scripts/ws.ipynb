{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import bnl\n",
    "from bnl import viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's plot!\n",
    "\n",
    "- Figure 1: how framesize affect the triplet metrics\n",
    "- Figure 2: how continuous compares with 0.5: scatter plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "tids = bnl.fio.salami_tids()\n",
    "tid = random.choice(tids)\n",
    "salami_hiers = bnl.fio.salami_ref_hiers(tid=tid)\n",
    "while len(salami_hiers) < 2:\n",
    "    tid = random.choice(tids)\n",
    "    salami_hiers = bnl.fio.salami_ref_hiers(tid=tid)\n",
    "\n",
    "hiers = {'-'.join([str(tid), str(i)]): h for i, h in enumerate(salami_hiers)}\n",
    "\n",
    "for tid, h in hiers.items():\n",
    "    print(tid)\n",
    "    h.expand(format='slm', always_include=False).plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Mir_eval implementation and my implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok. now I need to do this over all tracks that have 2 annotations and record all the times and results, for several configs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import bnl\n",
    "from bnl import metrics\n",
    "import os, pqdm\n",
    "import xarray as xr, numpy as np\n",
    "from pqdm.processes import pqdm\n",
    "from glob import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(\"./compare_implementation\", exist_ok=True)\n",
    "\n",
    "# Determine number of CPU cores to use\n",
    "n_jobs = max(1, os.cpu_count() - 1)  # Leave one CPU free\n",
    "\n",
    "# Process tracks in parallel\n",
    "tids = bnl.fio.salami_tids()\n",
    "pqdm(tids, metrics.time_salami_track, n_jobs=n_jobs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Time to collect\n",
    "track_results_fpaths = glob('./compare_implementation/*.nc')\n",
    "\n",
    "das = []\n",
    "for fp in tqdm(track_results_fpaths):\n",
    "    tid = os.path.basename(fp).split('.')[0]\n",
    "    # Load the data array and assign the tid as a coordinate\n",
    "    da = xr.open_dataarray(fp)\n",
    "    da = da.assign_coords(tid=tid)\n",
    "    da = da.expand_dims('tid')\n",
    "    das.append(da)\n",
    "\n",
    "# Concatenate all the data arrays along the 'tid' dimension\n",
    "final_da = xr.concat(das, dim='tid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Save the final data array to a NetCDF file\n",
    "final_da.to_netcdf('./compare_implementation.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alright, I can load from saved data now. Now let's do scatter plot to see how they differ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(bnl.viz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from bnl import viz\n",
    "\n",
    "# Load the data array from the NetCDF file\n",
    "results = xr.open_dataarray('./compare_implementation.nc')\n",
    "fig, axes = plt.subplots(2, 2, figsize=(4.5,4.5), constrained_layout=True)\n",
    "if not isinstance(axes, np.ndarray):\n",
    "    axes = np.array([axes])\n",
    "else:\n",
    "    axes = axes.ravel()\n",
    "\n",
    "\n",
    "viz.plot_scatter_frame_vs_continuous(\n",
    "    results, sel_dict=dict(output='run_time'), ax=axes[0]\n",
    ")\n",
    "axes[0].set_title('Run Time (seconds)')\n",
    "\n",
    "viz.plot_scatter_frame_vs_continuous(\n",
    "    results, sel_dict=dict(output='lm'), frame_sizes=[1,2], ax=axes[1], color_start_idx=3\n",
    ")\n",
    "axes[1].set_title('Coarsest L ($\\geq 1s$)')\n",
    "\n",
    "viz.plot_scatter_frame_vs_continuous(\n",
    "    results, sel_dict=dict(output='lm'), frame_sizes=[0.25, 0.5], ax=axes[2], color_start_idx=1\n",
    ")\n",
    "axes[2].set_title('Coarser L ($\\leq 0.5s$)')\n",
    "\n",
    "viz.plot_scatter_frame_vs_continuous(\n",
    "    results, sel_dict=dict(output='lm'), frame_sizes=[0.1], ax=axes[3], color_start_idx=0\n",
    ")\n",
    "axes[3].set_title('Default L ($0.1s$)')\n",
    "\n",
    "# Set the title for each subplot\n",
    "# Create the legend for the middle subplot into a row of handle, and plot the thin bar under the how figure\n",
    "axes[0].set_ylabel('Frame Based L-score')\n",
    "axes[2].set_ylabel('Frame Based L-score')\n",
    "axes[2].set_xlabel('Continuous L-score')\n",
    "axes[3].set_xlabel('Continuous L-score')\n",
    "\n",
    "# Plot the legend with bigger handles\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "for h in handles:\n",
    "    h.set_sizes([100])  # Adjust the size as needed\n",
    "    h.set_alpha(0.65)  # Set transparency for better visibility\n",
    "leg = fig.legend(handles, labels, \n",
    "           loc='lower center', ncol=3, fontsize=10, bbox_to_anchor=(0.5,-0.17)\n",
    "          )\n",
    "leg.set_title('L-score Frame Size (seconds)')\n",
    "\n",
    "# Set all the handles in the plot back to the smaller size.\n",
    "for ax in axes:\n",
    "    handles, _ = ax.get_legend_handles_labels()\n",
    "    for h in handles:\n",
    "        # Set it back!\n",
    "        h.set_sizes([2.5])  # Adjust the size as needed\n",
    "        h.set_alpha(0.3)  # Set transparency for better visibility\n",
    "    \n",
    "# Show the plot\n",
    "fig.set_constrained_layout_pads(w_pad=0, h_pad=0, hspace=0, wspace=0)\n",
    "# fig.savefig('../../../text/hier_metric/figs/compare_implementation.pdf', transparent=True, bbox_inches='tight')\n",
    "fig.savefig('./compare_implementation.pdf', transparent=True, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Depth!\n",
    "\n",
    "Let's get a track from adobe and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bnl, random\n",
    "from bnl import viz, fio\n",
    "tids = fio.salami_tids()\n",
    "random_tid = random.choice(tids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_id='1458'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_id='1458'\n",
    "salami_annos = fio.salami_ref_hiers(tid=bad_id)\n",
    "salami_adobe = fio.adobe_hiers(tid=bad_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = salami_adobe.plot(legend=5, text=False, legend_offset=2, figsize=(5,4.));\n",
    "fig.suptitle(f'Adobe Hierarchy {random_tid}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salami_annos[0].plot(legend=5, text=False, legend_offset=2, figsize=(5,1));\n",
    "salami_annos[0].relabel().plot(legend=5, text=False, legend_offset=2, figsize=(5,1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salami_annos[0].expand(always_include=False).relabel().plot(text=True, legend=6, legend_offset=1.5, figsize=(8, 5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bnl import metrics, fio\n",
    "bad_id='1010'\n",
    "salami_annos = fio.salami_ref_hiers(tid=bad_id)\n",
    "salami_adobe = fio.adobe_hiers(tid=bad_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.15095305442810059, (0.7366836355959089, 0.736543345875254, 0.7366134840559572))\n",
      "(0.2579917907714844, (0.7600089438142289, 0.8376851003977025, 0.7969588053590114))\n",
      "(0.7335591316223145, (0.7558285058507821, 0.834840011231004, 0.7933719332935493))\n",
      "(1.8798332214355469, (0.7541893548256449, 0.832430667153229, 0.7913808462020459))\n"
     ]
    }
   ],
   "source": [
    "ref= salami_annos[1]\n",
    "est= salami_annos[0]\n",
    "print(metrics.time_lmeasure(ref, est, frame_size=0))\n",
    "print(metrics.time_lmeasure(ref, est, frame_size=0.5))\n",
    "print(metrics.time_lmeasure(ref, est, frame_size=0.2))\n",
    "print(metrics.time_lmeasure(ref, est, frame_size=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salami_annos[1].labels, salami_annos[0].itvls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchy.lmeasure(salami_annos[1].itvls, salami_annos[1].labels, salami_annos[0].itvls, salami_annos[0].labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mir_eval\n",
    "from mir_eval import hierarchy\n",
    "hierarchy.lmeasure(salami_annos[1].itvls, salami_annos[1].labels, salami_adobe.itvls, salami_adobe.labels, frame_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salami_adobe.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salami_annos[0].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's investigate!\n",
    "\n",
    "- Transitivity, and effects of depth.\n",
    "- Window and its effects. (It's faster now so we probably don't need it. Let's check the original claim that over all frames are redundant)\n",
    "    - We can visualize this by looking at how window size affect the per time measures iota alpha and rho.\n",
    "- Monotonicity Meet and its effect.\n",
    "    - Again look at per time measures and how they change before and after.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tids = bnl.fio.salami_tids()\n",
    "hiers = bnl.fio.salami_ref_hiers(tid=tids[0])\n",
    "hiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiers[0].plot(text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
